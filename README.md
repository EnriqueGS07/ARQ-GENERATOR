# Architecture Generator

Sistema para generar diagramas Mermaid de arquitectura analizando repositorios Git mediante an√°lisis est√°tico y modelos de lenguaje local (LLM).

## üèóÔ∏è Arquitectura del Sistema

### Visi√≥n General

El sistema est√° dise√±ado como una aplicaci√≥n containerizada que se ejecuta en una instancia EC2, utilizando Docker Compose para orquestar dos servicios principales:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              EC2 Instance (t3.large)                     ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ         Docker Compose Network                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                                                   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  architecture-generator                    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - FastAPI Service (Puerto 8000)          ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Imagen: Docker Hub                      ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - M√≥dulos: api.py, extractor.py,          ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             processor.py                   ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚îÇ HTTP                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ                  ‚ñº                               ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ollama                                    ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - LLM Service (Puerto 11434)              ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Modelo: llama3.2:3b-instruct-q4_0      ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  - Volumen persistente para modelos        ‚îÇ  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Componentes Principales

#### 1. **architecture-generator** (Servicio FastAPI)

Servicio principal que expone la API REST para an√°lisis de repositorios.

**Caracter√≠sticas**:

- Framework: FastAPI (Python 3.11)
- Puerto: 8000
- Imagen: Almacenada en Docker Hub
- Timeout: 1200 segundos (20 minutos) para mantener conexiones activas

**M√≥dulos**:

1. **`api.py`**: Capa de API REST

   - Endpoint `POST /analyze`: Analiza repositorio y genera diagrama
   - Endpoint `GET /health`: Health check del servicio
   - Validaci√≥n de API key (opcional)
   - Manejo de CORS
   - Clonaci√≥n temporal de repositorios Git

2. **`extractor.py`**: An√°lisis est√°tico de repositorios

   - Extracci√≥n de estructura de directorios
   - Detecci√≥n de archivos de configuraci√≥n (package.json, pom.xml, etc.)
   - Identificaci√≥n de dependencias
   - Detecci√≥n de m√≥dulos y tecnolog√≠as
   - Filtrado de archivos irrelevantes (node_modules, .git, etc.)

3. **`processor.py`**: Generaci√≥n de diagramas Mermaid
   - Construcci√≥n de prompts optimizados para LLM
   - Comunicaci√≥n con Ollama API
   - Extracci√≥n y validaci√≥n de c√≥digo Mermaid
   - Manejo de errores y timeouts

**Variables de Entorno**:

- `OLLAMA_API_URL`: URL del servicio Ollama (default: `http://ollama:11434`)
- `OLLAMA_MODEL`: Modelo LLM a utilizar (default: `llama3.2:3b-instruct-q4_0`)
- `EC2_API_KEY`: API key opcional para autenticaci√≥n

#### 2. **ollama** (Servicio LLM)

Servicio de modelos de lenguaje local para generar diagramas Mermaid.

**Caracter√≠sticas**:

- Imagen: `ollama/ollama:latest`
- Puerto: 11434
- Modelo: `llama3.2:3b-instruct-q4_0` (quantizado, 3B par√°metros)
- Volumen persistente: `ollama-data` para almacenar modelos

**Modelo Utilizado**:

- **llama3.2:3b-instruct-q4_0**: Modelo quantizado optimizado para velocidad y eficiencia
- Tama√±o: ~2GB
- Ideal para instancias EC2 con recursos limitados (t3.large)

### Flujo de Procesamiento

```
1. Cliente ‚Üí POST /analyze
   {
     "repo_url": "https://github.com/user/repo.git",
     "depth": 1
   }

2. api.py
   ‚îú‚îÄ‚îÄ Valida API key (si est√° configurada)
   ‚îú‚îÄ‚îÄ Valida URL del repositorio
   ‚îî‚îÄ‚îÄ Clona repositorio en directorio temporal

3. extractor.py
   ‚îú‚îÄ‚îÄ Analiza estructura de directorios
   ‚îú‚îÄ‚îÄ Identifica archivos clave (pom.xml, package.json, etc.)
   ‚îú‚îÄ‚îÄ Detecta dependencias y tecnolog√≠as
   ‚îî‚îÄ‚îÄ Extrae m√≥dulos y componentes

4. processor.py
   ‚îú‚îÄ‚îÄ Construye prompt con estructura del repositorio
   ‚îú‚îÄ‚îÄ Llama a Ollama API (HTTP POST)
   ‚îú‚îÄ‚îÄ Espera respuesta del modelo LLM
   ‚îî‚îÄ‚îÄ Extrae c√≥digo Mermaid de la respuesta

5. api.py ‚Üí Retorna respuesta
   {
     "mermaid": "flowchart TD\n    A[...] --> B[...]"
   }

6. Limpieza: Elimina directorio temporal del repositorio
```

### Arquitectura de Red

**Comunicaci√≥n Interna**:

- Los contenedores se comunican a trav√©s de la red interna de Docker Compose
- `architecture-generator` accede a `ollama` mediante `http://ollama:11434`
- No se requiere exposici√≥n externa del puerto 11434 (solo para debugging)

**Comunicaci√≥n Externa**:

- Puerto 8000 expuesto para acceso al servicio FastAPI
- Security Group de EC2 debe permitir tr√°fico en puerto 8000
- CORS configurado para permitir cualquier origen

### Persistencia de Datos

**Vol√∫menes Docker**:

- `ollama-data`: Almacena modelos de Ollama de forma persistente
- `./tmp:/app/tmp`: Directorio temporal para clonaci√≥n de repositorios (montado desde host)

**Datos No Persistentes**:

- Repositorios clonados se eliminan despu√©s de cada an√°lisis
- No se almacena informaci√≥n de repositorios analizados

## üìÅ Estructura del Proyecto

```
ARQ-GENERATOR/
‚îú‚îÄ‚îÄ api.py                    # M√≥dulo 3: API FastAPI (endpoints REST)
‚îú‚îÄ‚îÄ extractor.py              # M√≥dulo 1: An√°lisis est√°tico de repositorios
‚îú‚îÄ‚îÄ processor.py              # M√≥dulo 2: Generaci√≥n de diagramas Mermaid
‚îú‚îÄ‚îÄ Dockerfile                # Definici√≥n de imagen Docker del servicio
‚îú‚îÄ‚îÄ docker-compose.yml        # Orquestaci√≥n de servicios Docker
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias Python
‚îú‚îÄ‚îÄ README.md                 # Esta documentaci√≥n
‚îî‚îÄ‚îÄ tmp/                      # Directorio temporal (volumen Docker)
```

### Separaci√≥n de Responsabilidades

El c√≥digo est√° organizado en tres m√≥dulos independientes:

1. **`extractor.py`**: L√≥gica de an√°lisis est√°tico

   - No depende de FastAPI ni Ollama
   - Funciones puras de an√°lisis de archivos
   - F√°cil de testear de forma aislada

2. **`processor.py`**: L√≥gica de generaci√≥n con LLM

   - Comunicaci√≥n con Ollama
   - Construcci√≥n de prompts
   - Extracci√≥n de c√≥digo Mermaid
   - Independiente de la API

3. **`api.py`**: Capa de presentaci√≥n
   - Endpoints REST
   - Validaci√≥n de entrada
   - Orquestaci√≥n de extractor y processor
   - Manejo de errores HTTP

## üê≥ Containerizaci√≥n

### Dockerfile

La imagen del servicio est√° basada en `python:3.11-slim` e incluye:

- Git para clonaci√≥n de repositorios
- Dependencias Python desde `requirements.txt`
- Los tres m√≥dulos del servicio (`api.py`, `extractor.py`, `processor.py`)
- Uvicorn como servidor ASGI

### Docker Compose

Orquesta dos servicios:

```yaml
services:
  architecture-generator:
    image: usuario-dockerhub/architecture-generator:latest
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_API_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:3b-instruct-q4_0
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
```

**Ventajas de esta arquitectura**:

- Aislamiento de servicios
- F√°cil escalabilidad horizontal
- Gesti√≥n simplificada de dependencias
- Versionado mediante im√°genes Docker Hub

## üîß Configuraci√≥n

### Requisitos de Infraestructura

**EC2 Instance**:

- Tipo: t3.large o superior (8GB+ RAM recomendado)
- Almacenamiento: 50GB m√≠nimo (para modelos de Ollama)
- Sistema Operativo: Ubuntu 22.04 LTS o Amazon Linux 2023
- Docker y Docker Compose instalados

**Security Group**:

- Puerto 8000 abierto para acceso al servicio
- Puerto 11434 opcional (solo para debugging de Ollama)

### Variables de Entorno

**docker-compose.yml**:

```yaml
environment:
  - OLLAMA_API_URL=http://ollama:11434
  - OLLAMA_MODEL=llama3.2:3b-instruct-q4_0
  - EC2_API_KEY=${EC2_API_KEY:-} # Opcional
```

**Configuraci√≥n mediante archivo .env**:

```bash
EC2_API_KEY=tu-api-key-secreta
```

### L√≠mites y Restricciones

- **Tama√±o m√°ximo de repositorio**: 100MB (configurable en `api.py`)
- **Profundidad de clonaci√≥n**: 1-3 niveles (configurable en request)
- **Timeout de Ollama**: 1200 segundos (20 minutos)
- **Tama√±o de √°rbol**: M√°ximo 300 l√≠neas (configurable en `extractor.py`)
- **Tama√±o de archivo**: M√°ximo 40KB por archivo analizado

## üì° API

### Endpoints

#### `POST /analyze`

Analiza un repositorio Git y genera un diagrama Mermaid.

**Request**:

```json
{
  "repo_url": "https://github.com/user/repo.git",
  "depth": 1
}
```

**Response**:

```json
{
  "mermaid": "flowchart TD\n    A[Frontend] --> B[API]..."
}
```

**Headers opcionales**:

- `X-API-Key`: API key si est√° configurada

#### `GET /health`

Health check del servicio.

**Response**:

```json
{
  "status": "ok",
  "ollama": "connected",
  "model": "llama3.2:3b-instruct-q4_0"
}
```

### Ejemplo de Uso

```bash
curl -X POST http://TU-EC2-IP:8000/analyze \
  -H "Content-Type: application/json" \
  -H "X-API-Key: tu-api-key" \
  -d '{
    "repo_url": "https://github.com/octocat/Hello-World.git",
    "depth": 1
  }'
```

## üöÄ Despliegue

### 1. Construir y Publicar Imagen

```bash
# Login en Docker Hub
docker login

# Construir imagen
docker build -t usuario-dockerhub/architecture-generator:latest .

# Publicar imagen
docker push usuario-dockerhub/architecture-generator:latest
```

### 2. Desplegar en EC2

```bash
# En EC2, crear directorio
mkdir -p ~/architecture-generator
cd ~/architecture-generator

# Crear docker-compose.yml (ver secci√≥n de configuraci√≥n)

# Descargar imagen
docker-compose pull

# Iniciar servicios
docker-compose up -d

# Descargar modelo de Ollama
docker exec ollama ollama pull llama3.2:3b-instruct-q4_0

# Verificar
curl http://localhost:8000/health
```

### 3. Actualizar Servicio

```bash
# Reconstruir y publicar nueva versi√≥n
docker build -t usuario-dockerhub/architecture-generator:latest .
docker push usuario-dockerhub/architecture-generator:latest

# En EC2, actualizar
docker-compose pull
docker-compose up -d
```

## üîç Monitoreo y Troubleshooting

### Comandos √ötiles

```bash
# Ver estado de contenedores
docker-compose ps

# Ver logs en tiempo real
docker-compose logs -f architecture-generator
docker-compose logs -f ollama

# Verificar espacio en disco
df -h
docker system df

# Verificar modelos de Ollama
docker exec ollama ollama list

# Health check
curl http://localhost:8000/health
```

### Problemas Comunes

**Puerto en uso**:

```bash
sudo lsof -i :8000
sudo lsof -i :11434
docker-compose down
```

**Espacio insuficiente**:

```bash
docker system prune -a --volumes -f
docker exec ollama ollama rm modelo-no-usado
```

**Ollama no responde**:

```bash
curl http://localhost:11434/api/tags
docker-compose restart ollama
```

## üí° Caracter√≠sticas T√©cnicas

### Optimizaciones

- **Modelo quantizado**: Reduce uso de memoria y acelera inferencia
- **An√°lisis selectivo**: Solo analiza archivos relevantes
- **Clonaci√≥n superficial**: Usa `depth=1` por defecto para repositorios grandes
- **Timeouts configurables**: Permite procesar repositorios complejos

### Seguridad

- API key opcional para autenticaci√≥n
- Validaci√≥n de URLs de repositorios (solo GitHub, GitLab, Bitbucket)
- Eliminaci√≥n autom√°tica de repositorios temporales
- CORS configurado para desarrollo (ajustar para producci√≥n)

### Escalabilidad

- Arquitectura modular permite escalar componentes independientemente
- Docker Compose facilita agregar m√°s instancias
- Modelos de Ollama compartidos mediante vol√∫menes persistentes
